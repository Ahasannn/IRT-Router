index,name,profile
1,qwen25_14b_instruct,"Qwen/Qwen2.5-14B-Instruct is released on 2024-09-19 by Alibaba Cloud’s Qwen Team. It is an instruction-tuned 14.7B-parameter decoder-only transformer (RoPE, SwiGLU, RMSNorm, GQA) with long-context support (up to 131,072 tokens; up to 8,192 generation). Designed for strong general instruction following, long-text understanding, structured data (tables) and structured outputs (e.g., JSON), with multilingual support (29+ languages). License: Apache-2.0."
2,phi3_mini_128k_instruct,"microsoft/Phi-3-mini-128k-instruct is released on 2024-04-23 by Microsoft. It is a lightweight 3.8B-parameter instruction model with a 128K-token context window, trained on Phi-3 datasets (synthetic + filtered public web) and post-trained with supervised fine-tuning plus preference optimization for instruction following and safety. Positioned for memory/compute-constrained and latency-sensitive deployments while retaining strong code/math/logic performance for its size. License: MIT."
3,llama31_8b_instruct,"meta-llama/Llama-3.1-8B-Instruct is released on 2024-07-23 by Meta. It is an 8B-parameter instruction-tuned model with a 128,000-token context length (knowledge cutoff: 2023-12), intended for general-purpose assistant/chat use and downstream application building. License: Meta Llama 3.1 Community License."
4,qwen25_3b_instruct,"Qwen/Qwen2.5-3B-Instruct is released on 2024-09-19 by Alibaba Cloud’s Qwen Team. It is an instruction-tuned 3.09B-parameter decoder-only transformer (RoPE, SwiGLU, RMSNorm, GQA) with 32,768-token context length and up to 8,192 generation tokens. Built for fast/cheap general instruction following, with emphasis on long-text generation, structured understanding, and structured outputs. License: qwen-research."
5,mistral_7b_instruct_v03,"mistralai/Mistral-7B-Instruct-v0.3 is released on 2024-05-22 by Mistral AI. It is an instruction-tuned 7B model; v0.3 updates include extended vocabulary to 32,768, v3 tokenizer support, and native function calling support. Maximum context size is 32,768 tokens. License: Apache-2.0."
